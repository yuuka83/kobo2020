{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題５\n",
    "1810083　井上悠香\n",
    "### １．\n",
    "下記のサンプルコードを実行してMNISTデータを学習してください。\n",
    "- simple_convnet.pyの実装も見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3004036337907534\n",
      "=== epoch:1, train acc:0.333, test acc:0.314 ===\n",
      "train loss:2.29753070768707\n",
      "train loss:2.2934908513730816\n",
      "train loss:2.286193506170246\n",
      "train loss:2.2797479415393416\n",
      "train loss:2.267680337361651\n",
      "train loss:2.259163511327569\n",
      "train loss:2.2385797346568395\n",
      "train loss:2.2115658948829697\n",
      "train loss:2.180043263824378\n",
      "train loss:2.153706632188956\n",
      "train loss:2.117097820853274\n",
      "train loss:2.085083500126893\n",
      "train loss:2.0086899961034472\n",
      "train loss:1.953220030579053\n",
      "train loss:1.8675188375933471\n",
      "train loss:1.8420145246922417\n",
      "train loss:1.7708813491894952\n",
      "train loss:1.684483521657271\n",
      "train loss:1.715490303606159\n",
      "train loss:1.4891440198890618\n",
      "train loss:1.4151166363713068\n",
      "train loss:1.297073944868229\n",
      "train loss:1.3256833934566177\n",
      "train loss:1.1617210718770778\n",
      "train loss:1.0363741015211678\n",
      "train loss:1.0644650024329296\n",
      "train loss:0.9766233480433981\n",
      "train loss:0.9266367348522422\n",
      "train loss:0.9213007605420478\n",
      "train loss:0.9832928412488345\n",
      "train loss:0.7956577015103499\n",
      "train loss:0.826479047803289\n",
      "train loss:0.8669039993330546\n",
      "train loss:0.7982888234300858\n",
      "train loss:0.6866092924427604\n",
      "train loss:0.657335660887281\n",
      "train loss:0.6454452871190733\n",
      "train loss:0.6635878199477502\n",
      "train loss:0.6093643023544351\n",
      "train loss:0.5710452917103456\n",
      "train loss:0.6513965697812609\n",
      "train loss:0.7388139514064835\n",
      "train loss:0.6506843876104318\n",
      "train loss:0.6054721629083477\n",
      "train loss:0.5076135548103577\n",
      "train loss:0.5684119921494942\n",
      "train loss:0.4108551690414004\n",
      "train loss:0.5080907725553192\n",
      "train loss:0.4091851931729263\n",
      "train loss:0.5097970111864022\n",
      "=== epoch:2, train acc:0.821, test acc:0.826 ===\n",
      "train loss:0.542202677360742\n",
      "train loss:0.5885511847222258\n",
      "train loss:0.5463379477550628\n",
      "train loss:0.4054857587776015\n",
      "train loss:0.5505737293743699\n",
      "train loss:0.4710851041249651\n",
      "train loss:0.4266548521641886\n",
      "train loss:0.43028895871336525\n",
      "train loss:0.2848445729288116\n",
      "train loss:0.2800415897465116\n",
      "train loss:0.34309519493102014\n",
      "train loss:0.4137004705193015\n",
      "train loss:0.3952964103033129\n",
      "train loss:0.56349169870465\n",
      "train loss:0.3660544351555219\n",
      "train loss:0.3127821939938618\n",
      "train loss:0.5104258172153068\n",
      "train loss:0.5029287888607841\n",
      "train loss:0.36870641785770336\n",
      "train loss:0.7715011795707731\n",
      "train loss:0.435771674827667\n",
      "train loss:0.5889937762569979\n",
      "train loss:0.47561331234215765\n",
      "train loss:0.34076680572973617\n",
      "train loss:0.4675407363562356\n",
      "train loss:0.4061522843625823\n",
      "train loss:0.4549006682405091\n",
      "train loss:0.5136969573349615\n",
      "train loss:0.38071306735818466\n",
      "train loss:0.380809562902485\n",
      "train loss:0.651011527319759\n",
      "train loss:0.3776266992197616\n",
      "train loss:0.4810192526340775\n",
      "train loss:0.2817891712979984\n",
      "train loss:0.38915489520960345\n",
      "train loss:0.31779468351371437\n",
      "train loss:0.36138896788922403\n",
      "train loss:0.49893797035987697\n",
      "train loss:0.3058484520637663\n",
      "train loss:0.4030284921069919\n",
      "train loss:0.4813823662521806\n",
      "train loss:0.45317313950231225\n",
      "train loss:0.24365998410635237\n",
      "train loss:0.5349400217044181\n",
      "train loss:0.3647515575584158\n",
      "train loss:0.45095336453396456\n",
      "train loss:0.376612304561678\n",
      "train loss:0.23990927709922044\n",
      "train loss:0.214016862138424\n",
      "train loss:0.3438122280197271\n",
      "=== epoch:3, train acc:0.877, test acc:0.867 ===\n",
      "train loss:0.32336900921610234\n",
      "train loss:0.3649313281040877\n",
      "train loss:0.3212943731570669\n",
      "train loss:0.3166053833445759\n",
      "train loss:0.36266595810318797\n",
      "train loss:0.3371098325467935\n",
      "train loss:0.2421272681011225\n",
      "train loss:0.25566247289620153\n",
      "train loss:0.2644618681088342\n",
      "train loss:0.49125450713978225\n",
      "train loss:0.31966570457889726\n",
      "train loss:0.4777308498710422\n",
      "train loss:0.23304016193285715\n",
      "train loss:0.3803514489323039\n",
      "train loss:0.3217474534403169\n",
      "train loss:0.24784424212539644\n",
      "train loss:0.26866317166501263\n",
      "train loss:0.2804659916683999\n",
      "train loss:0.22692815697631424\n",
      "train loss:0.3380262825149129\n",
      "train loss:0.4261399641663711\n",
      "train loss:0.20622574827918883\n",
      "train loss:0.22480753687594185\n",
      "train loss:0.28692092553659176\n",
      "train loss:0.3448063024066909\n",
      "train loss:0.2798281855206551\n",
      "train loss:0.33193926079094965\n",
      "train loss:0.3738818263460601\n",
      "train loss:0.3207962390603475\n",
      "train loss:0.3161503734541314\n",
      "train loss:0.4064137403572702\n",
      "train loss:0.2224784795329346\n",
      "train loss:0.31130686927371154\n",
      "train loss:0.17577774677927896\n",
      "train loss:0.27358438577209304\n",
      "train loss:0.1659020269729374\n",
      "train loss:0.21358056739914236\n",
      "train loss:0.2297238681394635\n",
      "train loss:0.2642414688470921\n",
      "train loss:0.12939044377923933\n",
      "train loss:0.3217490310274389\n",
      "train loss:0.26870872185528705\n",
      "train loss:0.20369280913978385\n",
      "train loss:0.2269345550882487\n",
      "train loss:0.2738611842260149\n",
      "train loss:0.3536203080684441\n",
      "train loss:0.26128976252900893\n",
      "train loss:0.267838289237579\n",
      "train loss:0.2364285801068689\n",
      "train loss:0.3337901293540571\n",
      "=== epoch:4, train acc:0.899, test acc:0.889 ===\n",
      "train loss:0.1651756447268536\n",
      "train loss:0.16576754416236303\n",
      "train loss:0.3540794040092878\n",
      "train loss:0.2350332080023366\n",
      "train loss:0.23478650853098343\n",
      "train loss:0.2733613426866216\n",
      "train loss:0.21232483790443535\n",
      "train loss:0.24197835157439385\n",
      "train loss:0.13804916950174462\n",
      "train loss:0.1970947806312828\n",
      "train loss:0.17711879267096253\n",
      "train loss:0.18201824723877447\n",
      "train loss:0.18930720753053587\n",
      "train loss:0.3240363746486217\n",
      "train loss:0.2047529264922536\n",
      "train loss:0.32658418185665217\n",
      "train loss:0.21235398772543174\n",
      "train loss:0.21475192960040573\n",
      "train loss:0.3877512516287258\n",
      "train loss:0.20063979333863194\n",
      "train loss:0.29217587886643837\n",
      "train loss:0.17411866767925616\n",
      "train loss:0.23940708187629225\n",
      "train loss:0.1807407244267864\n",
      "train loss:0.33199894097839006\n",
      "train loss:0.1874966784439907\n",
      "train loss:0.280735120560634\n",
      "train loss:0.265688731553247\n",
      "train loss:0.1478131091506622\n",
      "train loss:0.2303697078583563\n",
      "train loss:0.22827180906365296\n",
      "train loss:0.34945401056120395\n",
      "train loss:0.1876830129531123\n",
      "train loss:0.1899744107311599\n",
      "train loss:0.23268347289732227\n",
      "train loss:0.19182299151581866\n",
      "train loss:0.23151285777107944\n",
      "train loss:0.19315246272555794\n",
      "train loss:0.3181397559829743\n",
      "train loss:0.18557458162464807\n",
      "train loss:0.1671877119628904\n",
      "train loss:0.23523154319529252\n",
      "train loss:0.20583744639133414\n",
      "train loss:0.23374937902525328\n",
      "train loss:0.22846974223014319\n",
      "train loss:0.23149046463804665\n",
      "train loss:0.14643375671251302\n",
      "train loss:0.22703257818001663\n",
      "train loss:0.13812119708245948\n",
      "train loss:0.24939980085752747\n",
      "=== epoch:5, train acc:0.915, test acc:0.897 ===\n",
      "train loss:0.24937331132794455\n",
      "train loss:0.2849285678637872\n",
      "train loss:0.10438950583105815\n",
      "train loss:0.24856947722015946\n",
      "train loss:0.2496532803362766\n",
      "train loss:0.24890452545663094\n",
      "train loss:0.23785254618635773\n",
      "train loss:0.17922540835243683\n",
      "train loss:0.09102606762840824\n",
      "train loss:0.1713394911969889\n",
      "train loss:0.23844218419250804\n",
      "train loss:0.1890008424835952\n",
      "train loss:0.15642005633232328\n",
      "train loss:0.207344870760738\n",
      "train loss:0.13147309343003355\n",
      "train loss:0.1305857934737147\n",
      "train loss:0.2531469191453533\n",
      "train loss:0.17887939328544333\n",
      "train loss:0.1366528372915892\n",
      "train loss:0.22266234213148844\n",
      "train loss:0.14993870429650896\n",
      "train loss:0.1945546644791607\n",
      "train loss:0.22344716423685418\n",
      "train loss:0.1960847092655487\n",
      "train loss:0.1393747965827663\n",
      "train loss:0.1773809062821967\n",
      "train loss:0.23227096522934648\n",
      "train loss:0.1344453259461377\n",
      "train loss:0.083467378726733\n",
      "train loss:0.3439886024592668\n",
      "train loss:0.16017939622233698\n",
      "train loss:0.21886829083725118\n",
      "train loss:0.17133358293808457\n",
      "train loss:0.17794824501222845\n",
      "train loss:0.18495762703738816\n",
      "train loss:0.26981282639765436\n",
      "train loss:0.2708081443232504\n",
      "train loss:0.1373496826066636\n",
      "train loss:0.13316089636283432\n",
      "train loss:0.17722464108579572\n",
      "train loss:0.07022486590458464\n",
      "train loss:0.10985717078017357\n",
      "train loss:0.2121887243739271\n",
      "train loss:0.13738005720759727\n",
      "train loss:0.1771674715024386\n",
      "train loss:0.15849198849906768\n",
      "train loss:0.18199223233053574\n",
      "train loss:0.12033576399528091\n",
      "train loss:0.15615699884772127\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.91\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXhU5d3/8fc3k8keEgg7QQHFBa2CUqpFbK0ralFb27rVam3x12q1tdJKa12fPqW1Vh8fd612sQqoCFRpsSraxxWjLApICaiQIBACCWRPJvfvjxlgkkzCBHJykpnP67pyZc4255MDc3/POXPOfcw5h4iIJK8UvwOIiIi/VAhERJKcCoGISJJTIRARSXIqBCIiSU6FQEQkyXlWCMzsMTPbYmYftjPdzOweMys2s+VmdoxXWUREpH1eHhH8CTijg+mTgdGRn6nAAx5mERGRdnhWCJxz/wa2dTDLOcBfXNjbQL6ZDfEqj4iIxJbq47qHARuihksi4z5rPaOZTSV81EB2dvaxhx12WLcEFBFJFO+9995W59yAWNP8LAQWY1zM/i6ccw8DDwOMHz/eFRUVeZlLRCThmNmn7U3z86qhEmB41HAhsNGnLCIiScvPQjAfuDRy9dBxQKVzrs1pIRER8ZZnp4bM7Cngy0B/MysBbgaCAM65B4EFwJlAMVADXO5VFhERaZ9nhcA5d+FepjvgKq/WLyIi8dGdxSIiSU6FQEQkyakQiIgkORUCEZEkp0IgIpLkVAhERJKcCoGISJJTIRARSXIqBCIiSU6FQEQkyfnZDbWIiMRh7pJS7li4mo0VtQzNz2Ta6Ydy7rhhXfb+KgQiIj3Y3CWlTJ/zAbWNIQBKK2qZPucDgC4rBioEIiIecM5R39RMdX0TNQ0hahpCVDc0UdsQ2j1uz3CImoY942rq90xbuqGCpuaWz+yqbQxxx8LVKgQiIl3BOUdDqLlF41vdEKKmvin8e1cDXR81raGJ6voQtY1NLRrxmqhGvqahieaYz1yMLT01hay0AFlpqWSnh39npQXaFIFdNlbUdtEWUCEQkV6koak50hg3RTXQkYa5IURtQ8cNc3TDvqvhr2kIEepEi50WSCErPUB2WiqZaQGyI433kLwgmWmpu4ez0gKt5kvdPRxu8ANkp4enZQUDpAZiX7tTfsuBFFDRdjz5QLtPn+wUFQIR8YxzjuqGEDtqG9lR18iO2qao140t964b9jTM7e2FN4bib7BTU4zs9FaNbjDAwNyM8PCuBjpq73tXY717z7zVcFZagGA7DbZXYhWBjsbvCxUCEWmXc47axlC4Aa9rpLK2sZ1GPTy9xevaRnbUNe11bzuQYjEb4YLsNIb3zdrdiO9q0HedOmm9972rQd/VwKelxtFgN4cg1AihhqjfDRCqCf9uboT6RqhpiDFPU9Trxpavm2O9Z/S4dsbHWq4bqBCIJDDnHHWNzVENc8uGujLSWMeatmt8e+eod8kMBsjLDNInM5U+GUEG5KZz0IBs+mQG6ZOxZ3yfjBT6ptTTN1BDH6smx9WSkdJI0DVhzXUxGscYjWJjE9S10/jGbNAb9zTosaa7Zo+2vEFqOgTSIBCElOCe1y1+p0FqGqTntB2/a7l3H/Eo4x4qBCL7yetrvOsa95xaqYzRULdpwFtN29vplIxgSqTBDtInI5V+2WmMKMhu0bj3yQzSJz1A39R68q2GvJQacpqryHLVBBsqoK6y7c+26OEKqN+xfxsiuoHc9TolNcb4IATzIsPBTiyX1mqZGMumdPCegaj3TAns398aTYVApGeL5xrv+qZQp/fCo8c3NHW815oWSKFPZpC8zFT6ZAbJz0rjgIJs+mSkttwrT0+lb7Cevim15FFNDjVku6pWDXnU64pK2BQ9bQewl3P06X0gI2/PT/5wyDgyMpzfclp6LgQz29ljbtWwp6SC2X7/e0lsKgQi+6iuMcSkeV9kVaACWu0Als3N4/MLHmNHbSP1e2nIgwEL731nBMmN7JUP65tJn4xgy73yjFT6pjaQHwg35LlUk91cTVpT673xSOO9oxK2VEJtZLh+x95PhaTltGy0+xTCwCNaNuDRP5lRjXt6n67dE5aw7IFQvSX2+C6iQiCyF5W1jRRvqWLtliqKy6oo3hL+2bC9ho/TY1+5McAqOeXwgS1OufTJSCU/2ETflBryU2rIdTVku2qCjTuw+h2RBjtqD3xnJZS1auRdqOOwweyWDXXOYOh/aNtGu81PfrghD6hJ6HGmrfF8FfpXFyH8peqWnfW7G/ndP2VVlO2s3z1fWmoKo/pn87nCPL5+9EB4q/33/E3VTbC11fnz5qaOg6Rmtmy0cwZC/9EdN+C7f/cJn0YR6SQVAkkqoWbHhm01uxv5XQ3+2i1V7Kzf00jnpgcY299x4fAqxmTtYGRwG4PZSm79ZlJ2lMBnJfCfTR2vrKEKsgqg36jY58jbjOsTvtJEpJupEEhCqmsMsa6sendjvzbS4H+8tZqGUDNpNDLYtjEmq5LTc6s4ZEgFwwPbGNBcRm79JgI7S7HyGiiPetNAOuQVhn8OOjn8+7UZ7Yf43kue/50iXUGFQHq1mOfvN++kumIzQ9jKMCtnaMpWTsyo5LK0Cobmb6Vf0xYy67eG3yAEu2/QzB4YbtwHHQ6HnLan0c8rhLzhkN2/7ZUrHRUCkV5ChUB6vNbn7z/ZVE7Fpo9pKP+UrLpNDLOtDKWcL6Vs49LUbQx0ZaSlt74jMxOyhkca9fHhhj26oe8zbN9Oy3TDFR0iXlMhkB4j1OzYUF7F+vWfsnXjWqo2f0LT9vUEqzcyIFTGUNvKWVZOf4u6MSkIDiOUPYhA/nAsf8KePfjovfnMvt5ch94NV3SIeE2FQLpXQzX15evZvKGY7Z+to7bsU9hRQkbNZ/Rt3MwQyhlhLa+sqU/JpDZ7CC5vOOkFJ+D6H4jl72noLXcoqalpPv1BIr2fCoF0neZmqNoMlSVQuYG68k/ZsfkTGsvXE9hZQnbdJnKbd5AOHBD5CTmjPKUflWmDqepzFB/nDydr4AgKho0ie8AIyCskPSOfdN1VKuIZFQKJX31VpJEPN/RUluAqN9C4bQPNFRsIVn9GwO3Zm88AGl0m21x/NtGfqoyJhPoMI63vAeQOHsmgwoMZfuAoBmZkoDPqIv5RIZDYqrbA63fBto93N/hW1/Iu2hApbKYfJc0FbHTD2ejGUR4IX3mTOeBA+g0dxQFDhnDwwBwm9csikKK9epGeSIVA2mpupunpK7D1b/JZ8ABKmgtY2zCBDaF+bHT9KXUF1GcPJW/AcEYNyuPggTkcPDCH4wfmMDA3HdNpHJFeRYVA2nDvPkLqp//mF41X8HrmV3c39AcPyOHUyO+8LHVlIJIoVAikpa3FhBbexOuhozn4jKv570mj/E4kIh7z9OGbZnaGma02s2IzuyHG9APMbJGZLTGz5WZ2ppd5ZC9CTdQ+/X2qQwHmHnADl58w0u9EItINPCsEZhYA7gMmA2OAC81sTKvZbgRmO+fGARcA93uVR/au8f/uJnPz+/wu8D1+ecHJOtcvkiS8PCKYABQ759Y55xqAmcA5reZxQJ/I6zxgo4d5pCObPiDltd/wQmgCp3zjKgbkqhdMkWThZSEYBmyIGi6JjIt2C3CJmZUAC4AfxXojM5tqZkVmVlRWVuZF1uTWVM/OmVewrTmbFeNu4aTDB/mdSES6kZeFINZ5hdYPPL0Q+JNzrhA4E/irmbXJ5Jx72Dk33jk3fsCAAR5ETW7VL/4XuRWruTfnGq756nF+xxGRbuZlISgBhkcNF9L21M8VwGwA59xbhG9G7e9hJmml+dN3yFx8L882f5mLL72SjKCeOSuSbLwsBO8Co81spJmlEf4yeH6redYDJwOY2eGEC4HO/XSXhmp2zvoeG10/Gk/9NYcMyvU7kYj4wLNC4JxrAq4GFgKrCF8dtMLMbjOzKZHZfgp838yWAU8BlznnWp8+Eo9sm/cL8mrW89SQG/jWCUf4HUdEfOLpDWXOuQWEvwSOHndT1OuVwEQvM0hs9atfod+KPzEz5Uy+e8mlulRUJInpzuJkVFdJ7TNXUtI8hOHf+i0FObpUVCSZeXpnsfRMpTN/TG5DGf935O1MPPwAv+OIiM9UCJJMxZK5DPtkDk9nfpOLvvZ1v+OISA+gQpBEmneWYX+/lpVuBJ+/bAZpqfrnFxEVguThHJ/85UoyQlV8MulODhrcz+9EItJDqBAkiQ2v/ZlRZS/zQsHlTD75ZL/jiEgPokKQBGrL15P/6i9Ybofyle/erktFRaQFFYJE5xzrH7+CgGuiacr95Odk+p1IRHoYFYIE9+H8uzm0ajGvj7yGY8aN9zuOiPRAKgQJbPMnqxi15DcsCY7jpEum+x1HRHooFYIEFWpqYvvfvkuTC9D/4kcIpqpXURGJTYUgQb3z5G0c1riSVeNuZPiI0X7HEZEeTIUgAa1a9jbHrr2PpTmTmDDlB37HEZEeToUgwVTV1BCY+wNqLItRlz+CpeifWEQ6plYiwbz52A0c4tZRdtId9CkY4nccEekFVAgSyGuL/slXyv7KigFncciXLvA7joj0EioECaJkSznDX72OikA/Dr3sPr/jiEgvokKQAJpCzSz903WMslKap9xHanZfvyOJSC+iQpAA5j43i7Nr5rJu5EUMHHuG33FEpJdRIejl3v/Ppxz3wY2UBYcx6sLf+x1HRHohPbO4F9tR10jprOs42rZR963nIS3b70gi0gvpiKCXcs7x5F8f4auhlyj73FSyD57odyQR6aVUCHqpF95ZwddKfsvWrIMZfM5tfscRkV5Mp4Z6oU/Lqwn843r6WRV2yXxITfc7koj0Yjoi6GUaQ808/af/YbK9RfUXrycw9Gi/I4lIL6dC0Ms8uuBNvrfjXrb3PYq8k6f5HUdEEoBODfUib6/dymGLf0FWahNpFz8GAf3zicj+0xFBL1FZ08iip37PSYFluJNvhv56xoCIdA0Vgl7AOcfvZy/kR42Ps3PI8aR/Uc8YEJGuo3MLvcDT767n7HW3kxYMkPath0HPGBCRLqQWpYdbV1bFuufv4AspH5F61m8h/wC/I4lIglEh6MEampr5/RPz+UnKTOpGnUbKuEv8jiQiCUiFoAe7a+EK/t/232Fp2WScdy+Y+R1JRBKQviPood4o3kraW3dxVOrHcM6fIXeQ35FEJEF5ekRgZmeY2WozKzazG9qZ55tmttLMVpjZk17m6S22Vzfw8Mxn+VHqXJqOOB+OONfvSCKSwDw7IjCzAHAfcCpQArxrZvOdcyuj5hkNTAcmOue2m9lAr/L0Fs45fvl0ETc2/g8uuz/Bs/WMARHxlpdHBBOAYufcOudcAzATOKfVPN8H7nPObQdwzm3xME+v8OTi9RxdfC+jrZTgefdDph47KSLe8rIQDAM2RA2XRMZFOwQ4xMzeMLO3zSzmcxbNbKqZFZlZUVlZmUdx/Ve8ZScLnn+W76cuwB17OYw+xe9IIpIEvCwEsS5xca2GU4HRwJeBC4FHzSy/zULOPeycG++cGz9gwIAuD9oT1DeFmPbkW8wIPIjLOwA77b/8jiQiSSKuQmBmz5rZWWbWmcJRAgyPGi4ENsaYZ55zrtE59zGwmnBhSDq/++dqzt/6IIVsIfC1ByE9x+9IIpIk4m3YHwAuAtaY2QwzOyyOZd4FRpvZSDNLAy4A5reaZy5wEoCZ9Sd8qmhdnJkSxmv/KaP4zblcnPoy9sWr4cAv+h1JRJJIXIXAOfeSc+5i4BjgE+BfZvammV1uZsF2lmkCrgYWAquA2c65FWZ2m5lNicy2ECg3s5XAImCac658//6k3mVrVT23zHqDO9MfoXnAYXDSjX5HEpEkE/flo2ZWAFwCfBtYAvwNOAH4DuFz/G045xYAC1qNuynqtQOui/wkHeccP3tmOT9pfJiC1B3Yec9BMMPvWCKSZOIqBGY2BzgM+CvwVefcZ5FJs8ysyKtwie4vb31K+n/+zpS0N+DEX8DQsX5HEpEkFO8Rwb3OuVdiTXDOje/CPElj9aadPLTgLRZmPI4bPA6blJQHRSLSA8T7ZfHh0Zd1mllfM/uhR5kSXl1jiGuefJ8ZwUfJsXrsvIcgEPOrFhERz8VbCL7vnKvYNRC5E/j73kRKfL9ZsIqjyp/nRFeEnXIzDDjU70giksTiPTWUYmYW+XJ3Vz9Cad7FSlyvfLSZl94q4pWsJ2D4CfAFPXZSRPwVbyFYCMw2swcJ3x38/4B/epYqQW3ZWce02Uv5Y84fSUtJgXPv02MnRcR38RaCnwNXAj8g3HXEi8CjXoVKRM3Njp/OXsZ5jS8wNrAcvnoP9B3hdywRkfgKgXOumfDdxQ94GydxPfbGx5QWL+fxzJlw0OlwzKV+RxIRAeK/j2A08BtgDLD7jifn3CiPciWUFRsrufOfK3mhz6MEUrJgyj167KSI9BjxnqB+nPDRQBPhvoH+QvjmMtmL2oYQ1zy1hGvSn2dU/UfYWXdC7mC/Y4mI7BZvIch0zr0MmHPuU+fcLcBXvIuVOG5/YSXp5Su4kmfgiK/BkV/3O5KISAvxfllcF+mCeo2ZXQ2UAkn/WMm9WbhiE8+8s5bX+/6RlEA/OOtOvyOJiLQR7xHBj4Es4BrgWMKdz33Hq1CJYFNlHT9/djm/zv87A2vXwpR7Iauf37FERNrY6xFB5OaxbzrnpgFVwOWep+rlmpsd181eyuGNqzjfzQlfIXTIaX7HEhGJaa+FwDkXMrNjo+8slo49/H/rWLK2lMX9HsHSCuH0//Y7kohIu+L9jmAJMM/Mngaqd410zs3xJFUvtrykgt8vXM0fB80jt3IDfON5SM/1O5aISLviLQT9gHJaXinkABWCKNX1TVw7cylnZq/iS5Xz4LgfwshJfscSEelQvHcW63uBONz69xWUl2/hjr4PQ94hcPJNe19IRMRn8d5Z/DjhI4AWnHPf7fJEvdQLyz9jdlEJLxTOIb28DC5+EoKZfscSEdmreE8NPR/1OgM4D9jY9XF6p9KKWqbPWc6VA1dyxNZ/wIk/g2HH+h1LRCQu8Z4aejZ62MyeAl7yJFEvE2p2/GTWUvKaK/hZ4wMw+Cg4cZrfsURE4hbvEUFro4EDujJIb/XAq8Us/ricN0bMIlC2E857CFL1zB4R6T3i/Y5gJy2/I9hE+BkFSW3J+u3c9dIabh/xIcM2vQyn3gaDxvgdS0SkU+I9NaQL4VvZWdfItTOXclRuFZdsvxcOOB6Ov9rvWCIinRZXX0Nmdp6Z5UUN55vZud7F6vlunreC0u1V/Lnfn7DmZjj3fkgJ+B1LRKTT4u107mbnXOWuAedcBXCzN5F6vnlLS5mzpJRHDl9Gn8/egNNuh356Ro+I9E7xFoJY8+3rF8292oZtNdz43IecPayGk9bfCwedDON1O4WI9F7xFoIiM/uDmR1kZqPM7C7gPS+D9URNoWZ+PGspKTRzZ/BBLDUNzrlXj50UkV4t3kLwI6ABmAXMBmqBq7wK1VP97yvFvPfpdp48YjHpm4rgzN9Dn6F+xxIR2S/xXjVUDdzgcZYereiTbfzvK2u4akwdR6y+Fw6fAp/7ht+xRET2W7xXDf3LzPKjhvua2ULvYvUslbXhS0VH5Ae5ruoPkJEHZ9+lU0IikhDi/cK3f+RKIQCcc9vNLCmeWeyc48a5H7JpRx1vfP4NAss+hAueguz+fkcTEekS8X5H0Gxmu7uUMLMRxOiNNBHNeb+Uvy/byG+/UM/g5ffD2IvhsDP9jiUi0mXiPSL4JfC6mb0WGT4RmOpNpJ7jk63V3DTvQyaNyOLr62+CPsPgjN/4HUtEpEvFdUTgnPsnMB5YTfjKoZ8SvnIoYTWGmrl21lICKcYDg5/HyovhnPvC3w+IiCSQeL8s/h7wMuEC8FPgr8AtcSx3hpmtNrNiM2v3qiMzO9/MnJmNjy+29+5+6T8s21DBI5NqyFn6KEy4EkZ9ye9YIiJdLt7vCK4FPg986pw7CRgHlHW0gJkFgPuAycAY4EIza9M1p5nlAtcA73Qit6feXlfO/a+u5Tvj+vKF5b+CgoPhlFv8jiUi4ol4C0Gdc64OwMzSnXMfAYfuZZkJQLFzbp1zrgGYCZwTY77bgd8BdXFm8VRFTQM/mbWUEQXZ3Bj8K+wohXMfhLQsv6OJiHgi3kJQErmPYC7wLzObx94fVTkM2BD9HpFxu5nZOGC4cy76UZhtmNlUMysys6Kysg4PRPaLc47pcz5ga1U9jx+/leDyJ+GEn8Dwz3u2ThERv8V7Z/F5kZe3mNkiIA/4514Wi3W31e5LTs0sBbgLuCyO9T8MPAwwfvx4zy5bnV20gX98uIlbTx7EiDcvhEGfgy8l9Q3VIpIEOt2DqHPutb3PBYSPAIZHDRfS8igiFzgSeNXCd+gOBuab2RTnXFFnc+2vtWVV3DJ/JRMP6sel2+6B2u3w7ef02EkRSXjxnhraF+8Co81spJmlARcA83dNdM5VOuf6O+dGOOdGAG8DvhSBhqZmrp25hIxgCvcdtQ5bNQ9O+gUMPrK7o4iIdDvPCoFzrgm4GlgIrAJmO+dWmNltZjbFq/XuiztfXM2HpTu4+8xB5C+aDoUTYOK1fscSEekWnj5cxjm3AFjQatxN7cz7ZS+ztOf1NVt56N/ruHjCcL700a0QaoTzHtRjJ0UkaXh5aqjH21bdwHWzl3LwwBxuHvouFL8Ep94GBQf5HU1EpNskbSFwzvGzZ5ZTUdPIA2f2Je3lX8GoL8P4K/yOJiLSrZK2EPztnfW8tGozN5wxmtFv/gxSUsN9CaUk7SYRkSSVlA+gX7N5J7c/v5ITDxnAZbYA1r8Vvns4r9DvaCIi3S7pdn/rGkNcM3MpOemp3HVSGimLbofDzoajL/A7moiIL5LiiGDuklLuWLiajRW1ZKUHqK4P8filR1Pw4kWQ3gfOvluPnRSRpJXwhWDuklKmz/mA2sYQANX1IQIpxpBl98Jny+BbT0DOAJ9Tioj4J+FPDd2xcPXuIrDLEa6Yg1c/BEddAId/1adkIiI9Q8IXgo0VLR+klk4Dfwg+QJnLh8m/9SmViEjPkfCFYGh+ZovhaamzODhlI79L+xFk5vuUSkSk50j47whedt8jI6O8zfjfpNwLXNf9gUREepiEPyLIqG9bBDoaLyKSbBK+EIiISMdUCEREkpwKgYhIklMhEBFJcolfCLIHdm68iEiSSfjLR5m2xu8EIiI9WuIfEYiISIdUCEREkpwKgYhIklMhEBFJcioEIiJJToVARCTJqRCIiCQ5FQIRkSSnQiAikuRUCEREkpwKgYhIklMhEBFJcioEIiJJToVARCTJqRCIiCQ5FQIRkSTnaSEwszPMbLWZFZvZDTGmX2dmK81suZm9bGYHeplHRETa8qwQmFkAuA+YDIwBLjSzMa1mWwKMd84dBTwD/M6rPCIiEpuXRwQTgGLn3DrnXAMwEzgnegbn3CLnXE1k8G2g0MM8IiISg5eFYBiwIWq4JDKuPVcA/4g1wcymmlmRmRWVlZV1YUQREfGyEFiMcS7mjGaXAOOBO2JNd8497Jwb75wbP2DAgC6MKCIiqR6+dwkwPGq4ENjYeiYzOwX4JfAl51y9h3lERCQGL48I3gVGm9lIM0sDLgDmR89gZuOAh4ApzrktHmYREZF2eFYInHNNwNXAQmAVMNs5t8LMbjOzKZHZ7gBygKfNbKmZzW/n7URExCNenhrCObcAWNBq3E1Rr0/xcv0iIrJ3nhYCEZGeorGxkZKSEurq6vyO4qmMjAwKCwsJBoNxL6NCICJJoaSkhNzcXEaMGIFZrIsaez/nHOXl5ZSUlDBy5Mi4l1NfQyKSFOrq6igoKEjYIgBgZhQUFHT6qEeFQESSRiIXgV325W9UIRARSXIqBCIiMcxdUsrEGa8w8oYXmDjjFeYuKd2v96uoqOD+++/v9HJnnnkmFRUV+7XuvVEhEBFpZe6SUqbP+YDSilocUFpRy/Q5H+xXMWivEIRCoQ6XW7BgAfn5+fu83njoqiERSTq3/n0FKzfuaHf6kvUVNISaW4yrbQzxs2eW89Ti9TGXGTO0Dzd/9Yh23/OGG25g7dq1jB07lmAwSE5ODkOGDGHp0qWsXLmSc889lw0bNlBXV8e1117L1KlTARgxYgRFRUVUVVUxefJkTjjhBN58802GDRvGvHnzyMzM3Ict0JKOCEREWmldBPY2Ph4zZszgoIMOYunSpdxxxx0sXryYX//616xcuRKAxx57jPfee4+ioiLuueceysvL27zHmjVruOqqq1ixYgX5+fk8++yz+5wnmo4IRCTpdLTnDjBxxiuUVtS2GT8sP5NZVx7fJRkmTJjQ4lr/e+65h+eeew6ADRs2sGbNGgoKClosM3LkSMaOHQvAscceyyeffNIlWXREICLSyrTTDyUzGGgxLjMYYNrph3bZOrKzs3e/fvXVV3nppZd46623WLZsGePGjYt5L0B6evru14FAgKampi7JoiMCEZFWzh0XfobWHQtXs7GilqH5mUw7/dDd4/dFbm4uO3fujDmtsrKSvn37kpWVxUcffcTbb7+9z+vZFyoEIiIxnDtu2H41/K0VFBQwceJEjjzySDIzMxk0aNDuaWeccQYPPvggRx11FIceeijHHXdcl603HuZczIeG9Vjjx493RUVFfscQkV5m1apVHH744X7H6Bax/lYze885Nz7W/PqOQEQkyakQiIgkORUCEZEkp0IgIpLkVAhERJKcCoGISJLTfQQiIq3dMRqqt7Qdnz0Qpq3Zp7esqKjgySef5Ic//GGnl7377ruZOnUqWVlZ+7TuvdERgYhIa7GKQEfj47CvzyOAcCGoqanZ53XvjY4IRCT5/OMG2PTBvi37+Fmxxw/+HEye0e5i0d1Qn3rqqQwcOJDZs2dTX1/Peeedx6233kp1dTXf/OY3KSkpIRQK8atf/YrNmzezceNGTjrpJPr378+iRYv2LXcHVAhERLrBjBkz+PDDD1m6dCkvvvgizzzzDIsXL8Y5x5QpU/j3v/9NWVkZQ4cO5YUXXgDCfRDl5eXxhz/8gUWLFtG/f39PsqkQiEjy6WDPHYBb8tqfdvkL+736F198kRdffJFx48YBUFVVxSzVuHkAAAflSURBVJo1a5g0aRLXX389P//5zzn77LOZNGnSfq8rHioEIiLdzDnH9OnTufLKK9tMe++991iwYAHTp0/ntNNO46abbvI8j74sFhFpLXtg58bHIbob6tNPP53HHnuMqqoqAEpLS9myZQsbN24kKyuLSy65hOuvv57333+/zbJe0BGBiEhr+3iJaEeiu6GePHkyF110EccfH37aWU5ODk888QTFxcVMmzaNlJQUgsEgDzzwAABTp05l8uTJDBkyxJMvi9UNtYgkBXVDrW6oRUSkHSoEIiJJToVARJJGbzsVvi/25W9UIRCRpJCRkUF5eXlCFwPnHOXl5WRkZHRqOV01JCJJobCwkJKSEsrKyvyO4qmMjAwKCws7tYwKgYgkhWAwyMiRI/2O0SN5emrIzM4ws9VmVmxmN8SYnm5msyLT3zGzEV7mERGRtjwrBGYWAO4DJgNjgAvNbEyr2a4AtjvnDgbuAn7rVR4REYnNyyOCCUCxc26dc64BmAmc02qec4A/R14/A5xsZuZhJhERacXL7wiGARuihkuAL7Q3j3OuycwqgQJga/RMZjYVmBoZrDKz1fuYqX/r9+4hlKtzlKvzemo25eqc/cl1YHsTvCwEsfbsW1+3Fc88OOceBh7e70BmRe3dYu0n5eoc5eq8nppNuTrHq1xenhoqAYZHDRcCG9ubx8xSgTxgm4eZRESkFS8LwbvAaDMbaWZpwAXA/FbzzAe+E3l9PvCKS+S7PUREeiDPTg1FzvlfDSwEAsBjzrkVZnYbUOScmw/8EfirmRUTPhK4wKs8Eft9eskjytU5ytV5PTWbcnWOJ7l6XTfUIiLStdTXkIhIklMhEBFJcglZCHpq1xZx5LrMzMrMbGnk53vdlOsxM9tiZh+2M93M7J5I7uVmdkwPyfVlM6uM2l6eP+XbzIab2SIzW2VmK8zs2hjzdPv2ijOXH9srw8wWm9mySK5bY8zT7Z/HOHP58nmMrDtgZkvM7PkY07p+eznnEuqH8BfTa4FRQBqwDBjTap4fAg9GXl8AzOohuS4D7vVhm50IHAN82M70M4F/EL7v4zjgnR6S68vA8928rYYAx0Re5wL/ifHv2O3bK85cfmwvA3Iir4PAO8Bxrebx4/MYTy5fPo+RdV8HPBnr38uL7ZWIRwQ9tWuLeHL5wjn3bzq+f+Mc4C8u7G0g38yG9IBc3c4595lz7v3I653AKsJ3yEfr9u0VZ65uF9kGVZHBYOSn9RUq3f55jDOXL8ysEDgLeLSdWbp8eyViIYjVtUXrD0SLri2AXV1b+J0L4OuR0wnPmNnwGNP9EG92PxwfObz/h5kd0Z0rjhySjyO8NxnN1+3VQS7wYXtFTnMsBbYA/3LOtbu9uvHzGE8u8OfzeDfwM6C5neldvr0SsRB0WdcWXSyedf4dGOGcOwp4iT1V329+bK94vA8c6Jw7GvhfYG53rdjMcoBngR8753a0nhxjkW7ZXnvJ5cv2cs6FnHNjCfcuMMHMjmw1iy/bK45c3f55NLOzgS3Oufc6mi3GuP3aXolYCHpq1xZ7zeWcK3fO1UcGHwGO9ThTvOLZpt3OObdj1+G9c24BEDSz/l6v18yChBvbvznn5sSYxZfttbdcfm2vqPVXAK8CZ7Sa5GtXM+3l8unzOBGYYmafED59/BUze6LVPF2+vRKxEPTUri32mqvVeeQphM/z9gTzgUsjV8McB1Q65z7zO5SZDd51btTMJhD+/1zu8TqN8B3xq5xzf2hntm7fXvHk8ml7DTCz/MjrTOAU4KNWs3X75zGeXH58Hp1z051zhc65EYTbiFecc5e0mq3Lt1fCParS9cyuLeLNdY2ZTQGaIrku8zoXgJk9RfiKkv5mVgLcTPjLM5xzDwILCF8JUwzUAJf3kFznAz8wsyagFrigGwr6RODbwAeR88sAvwAOiMrlx/aKJ5cf22sI8GcLP6gqBZjtnHve789jnLl8+TzG4vX2UhcTIiJJLhFPDYmISCeoEIiIJDkVAhGRJKdCICKS5FQIRESSnAqBiMcs3Otnm14kRXoKFQIRkSSnQiASYWaXRPqoX2pmD0U6JasyszvN7H0ze9nMBkTmHWtmb0c6JHvOzPpGxh9sZi9FOnZ738wOirx9TqTjso/M7G9Rd/jOMLOVkff5vU9/uiQ5FQIRwMwOB74FTIx0RBYCLgaygfedc8cArxG+uxngL8DPIx2SfRA1/m/AfZGO3b4I7OpaYhzwY2AM4WdSTDSzfsB5wBGR9/kvb/9KkdhUCETCTibcqdi7kS4aTibcYDcDsyLzPAGcYGZ5QL5z7rXI+D8DJ5pZLjDMOfccgHOuzjlXE5lnsXOuxDnXDCwFRgA7gDrgUTP7GuHuKES6nQqBSJgBf3bOjY38HOqcuyXGfB31ydLRw0Hqo16HgNRIX/ITCPcYei7wz05mFukSKgQiYS8D55vZQAAz62dmBxL+jJwfmeci4HXnXCWw3cwmRcZ/G3gt0v9/iZmdG3mPdDPLam+FkWcH5EW6hP4xMNaLP0xkbxKu91GRfeGcW2lmNwIvmlkK0AhcBVQDR5jZe4SfBPWtyCLfAR6MNPTr2NPD6LeBhyK9RTYC3+hgtbnAPDPLIHw08ZMu/rNE4qLeR0U6YGZVzrkcv3OIeEmnhkREkpyOCEREkpyOCEREkpwKgYhIklMhEBFJcioEIiJJToVARCTJ/X9R+CQm7vMtsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 5 #20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感想\n",
    "CNNを用いることで、画像の形状を保ったままでも学習できた。今回のデータには関係ないが、これでRGBなどのチャンネルも加味した学習ができるようになったので、色のついた画像などで今までできなかった学習の仕方ができるようになると考えられる。今回の認識精度は91%であった。高い値ではあるが、前回より小さい値であった。グラフを見ると、testとtrainの認識精度が少し差が出始めている。おそらくこのデータセットに対する学習を続けても過学習が起きてしまうと考えられる。そこでもっと認識精度をあげるために前処理などにもっと工夫がいると思った。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 7章まとめ（simple_convnet.pyの実装について）\n",
    "7章では、畳み込みニューラルネットワーク（convolitional neural network:CNN）を学んだ。まず、CNNとは、画像認識や音声認識など至る所で使われているネットワークである。これまで見てきた全結合のニューラルネットワークでは,全結合層（Affineレイヤ）を用いていた。しかし、問題点として、\n",
    "「データの形状が無視されてしまう」ことが挙げられる。<br>画像は通常、縦・横・チャンネルの三次元の形状である（チャンネル例：RGB）。そのため、全結合層に入力しようとすると一次元の形状にしなければならない。MINSTでは、28x28の画像を784x1にして入力していた。<br>\n",
    "CNNでは、形状の情報を維持したまま入力できるので形状の情報を活かすことができる可能性がある。<br>\n",
    "ここにCNNについてまとめていく。<br>\n",
    "**構造**\n",
    "- CNNは新しく「Convolutionレイヤ」と「Pooling レイヤ」が加わる。\n",
    "- レイヤの繋がり順は「Convolution-ReLU-(Pooling)」\n",
    "- 出力に近い層では、「Affine・ReLU」が使われている。\n",
    "- 一番最後の出力層では、「Affine-Softmax」の組み合わせ \n",
    "\n",
    "**用語**\n",
    "- 特徴マップ(feature map)：畳み込み層の入出力データ\n",
    "- 入力特徴マップ(imput feature map)：畳み込み層の入力データ\n",
    "- 出力特徴マップ（output feature map）：出力特徴マップ\n",
    "\n",
    "**畳み込み演算**\n",
    "- 画像処理でいう「フィルター処理」のこと\n",
    "- 三次元データの畳み込み演算では、チャンネル方向ごとに演算して最後に加算して一つの出力を得る\n",
    "\n",
    "**パディング**\n",
    "- 畳み込み演算を行う時端っこにはピクセルが存在しなくなってしまう問題を、穴埋めすることで解決すること\n",
    "\n",
    "**ストライド**\n",
    "- フィルターに適用する位置の感覚\n",
    "\n",
    "**プーリング層**\n",
    "- 縦・横方向の空間を小さくする演算\n",
    "- 学習するパラメータがない\n",
    "  - プーリング層では、対象領域から最大値を取る / 平均値をとるだけの処理なので学習するべきパラメータはない\n",
    "- チャンネル数は変化しない\n",
    "  - プーリングの演算によって入力データと出力データのチャンネル数は変化しない。チャンネルごとに独立して研鑽される\n",
    "- 微小な位置変化に対してロバスト（頑強）\n",
    "  - 入力データの小さなズレに対してプーリング層は同じような結果を返す。\n",
    "  - 入力データのズレをプーリングが吸収する\n",
    "  \n",
    "**ConvolutionレイヤとPooling レイヤ実装について**\n",
    "- im2colという関数を使用する\n",
    "- im2colは、フィルターにとって都合のいいように入力データを展開する関数\n",
    "- フィルターを適用する場所の入力フィルターを二次元データにする\n",
    "- im2colは、Image to columnの略記\n",
    "- Convolutionレイヤの逆伝播にはcol2imを使って逆の操作をする\n",
    "- Poolingレイヤ\n",
    "  - 1. 入力データを展開\n",
    "  - 2. 行ごとに最大値を求める\n",
    "  - 3. 適切な出力サイズに整形する\n",
    "  -  im2col出直したら一行ずつ最大値を取って最後二次元を直す処理のみ\n",
    "  \n",
    "CNNを一言でまとめると、形状データも扱えるようにしたニューラルネットワーク。<br>\n",
    "そうを深くするごとに高度な情報が得られる。<br>\n",
    "代表的なCNNとしては、\n",
    "\n",
    "- LeNet：手書き数字認識を行うネットワークとして1998年に提案された。ReLUではなくSigmoid関数が使われた\n",
    "- AlexNet：ディープラニングの注目を集めたネットワーク。活性化関数としてReLUが使用された。LRNという層を用いている。DropOutを使用する\n",
    "\n",
    "がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting co-occurrence . . . \n",
      "calculating SVD . . .\n",
      "\n",
      "[query] you\n",
      " i: 0.7003179788589478\n",
      " we: 0.6367184519767761\n",
      " anybody: 0.5657642483711243\n",
      " do: 0.563567042350769\n",
      " 'll: 0.5127798318862915\n",
      "\n",
      "[query] year\n",
      " month: 0.6961644887924194\n",
      " quarter: 0.6884942054748535\n",
      " earlier: 0.6663320064544678\n",
      " last: 0.628136396408081\n",
      " next: 0.6175755858421326\n",
      "\n",
      "[query] car\n",
      " luxury: 0.672883152961731\n",
      " auto: 0.6452109813690186\n",
      " vehicle: 0.6097723245620728\n",
      " cars: 0.6032834053039551\n",
      " corsica: 0.5698372721672058\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7585657835006714\n",
      " nissan: 0.7148030996322632\n",
      " motors: 0.692615807056427\n",
      " lexus: 0.6583304405212402\n",
      " honda: 0.6350275278091431\n",
      "\n",
      "[query] we\n",
      " i: 0.7858525514602661\n",
      " you: 0.6367184519767761\n",
      " 're: 0.6329208612442017\n",
      " 've: 0.6128445863723755\n",
      " 'm: 0.6121340990066528\n",
      "\n",
      "[query] happy\n",
      " convinced: 0.6081444025039673\n",
      " confident: 0.5931104421615601\n",
      " sorry: 0.5903697609901428\n",
      " glad: 0.5726175904273987\n",
      " pleased: 0.56303870677948\n",
      "\n",
      "[query] this\n",
      " last: 0.5095800161361694\n",
      " year: 0.48587995767593384\n",
      " the: 0.47236838936805725\n",
      " another: 0.4323185682296753\n",
      " carolinas: 0.4277057945728302\n",
      "\n",
      "[query] win\n",
      " find: 0.5765163898468018\n",
      " convince: 0.5137775540351868\n",
      " persuade: 0.5082975625991821\n",
      " scuttle: 0.503400981426239\n",
      " pick: 0.49655675888061523\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('. .')\n",
    "import numpy as np\n",
    "from common.util import preprocess,create_co_matrix,most_similar,ppmi\n",
    "from dataset import ptb\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus,word_to_id,id_to_word = ptb.load_data('train')#corpusには単語IDのリストが格納される、ptb.load_Data()でデータをロードする\n",
    "vocab_size = len(word_to_id)#サイズ計算\n",
    "print('counting co-occurrence . . . ')\n",
    "C = create_co_matrix(corpus,vocab_size,window_size)#共起行列を求める\n",
    "print('calculating SVD . . .')\n",
    "W = ppmi(C)#ppmi行列を計算、途中経過を表示する\n",
    "try:\n",
    "    #truncated SVD　速いSVD\n",
    "    from sklearn.utils.exmath import randomized_svd\n",
    "    U,S,V = randomize_svd(W,n_components = wordvec_size,n_iter = 5,random_state = None)\n",
    "except ImportError:\n",
    "    #SVD　さっき使った方\n",
    "    U,S,V = np.linalg.svd(W)\n",
    "    \n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you','year','car','toyota','we','happy','this','win']\n",
    "for query in querys:\n",
    "    most_similar(query,word_to_id,id_to_word,word_vecs,top = 5)#類似度を計算\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  感想\n",
    "PTBデータをカウントベースの手法を用いて類似度を計算して与えたクエリと近い物を調べた。本とは別にのクエリとして、「we」「happy」「this」「win」も計算した。weの一番類似度が高いものは「i」で、その値も0.7858525514602661とかなり高い。主語の部分の言葉は感覚的にも一致するものが多く見られた。難しいなと感じたのは、動詞で、winに一番近いものはfindでその値は0.5765163898468018 でそこまで高くない。文脈から一番近いものを推測するのにカウントベースの手法だとうまくいかないこともありそうだと思った。今回思ったこととして、英語は単語単語の切れ目にスペースがあるので簡単に語彙に分けることができるが、日本語だとそうもいかないのでそこがまた難しそうだと思った。現時点で日本語は扱っていないけど、目標は日本語を人工知能を使って作ることなのでこの問題は心に留めておきたい。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
