{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "repo12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa7TJVsxtyYe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "9e291db1-bdc7-4383-8e4f-e4e33c5bbd6e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd '/content/gdrive/My Drive/Colab Notebooks/kobo2020'\n",
        "%ls\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks/kobo2020\n",
            "AttentionSeq2seq.pkl                neko.txt\n",
            "attention_seq2seq.py                nlp100_chap9\n",
            "BetterRnnlm.pkl                     nlp100_chap9.pkl\n",
            "better_rnnlm.py                     nlp100_chap9.txt\n",
            "cbow_params.pkl                     \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
            "cbow.py                             PyTorchTutorial1.ipynb\n",
            "\u001b[01;34mch04\u001b[0m/                               repo10.ipynb\n",
            "\u001b[01;34mch06\u001b[0m/                               repo10test.ipynb\n",
            "\u001b[01;34mch07\u001b[0m/                               repo11.ipynb\n",
            "ch07＿pra.ipynb                     repo11_test.ipynb\n",
            "\u001b[01;34mch08\u001b[0m/                               repo12.ipynb\n",
            "ch08_pra.ipynb                      repo6.ipynb\n",
            "\u001b[01;34mcommon\u001b[0m/                             repo7.ipynb\n",
            "\u001b[01;34mdata70\u001b[0m/                             repo8.ipynb\n",
            "\u001b[01;34mdatarepo11\u001b[0m/                         repo9.ipynb\n",
            "\u001b[01;34mdataset\u001b[0m/                            simple_rnnlm.py\n",
            "dev.tsv                             \u001b[01;34msmall_parallel_enja\u001b[0m/\n",
            "enword2id.pickle                    test_embedding_lstm.ipynb\n",
            "GoogleNews-vectors-negative300.bin  test.tsv\n",
            "jaword2id.pickle                    \u001b[01;34mtext\u001b[0m/\n",
            "ldcc-20140209.tar.gz                train.tsv\n",
            "ldcc-20140209.tar.gz.1              Untitled0.ipynb\n",
            "negative_sampling_layer.py          wakaranmemo.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKr4N7cM2Jcg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "0013d531-388f-4267-d8dc-c9bc7131351d"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"kobo_bert.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1pwU35uTpZq29_4kCFPfrTaC2mvo_eZnK\n",
        "\"\"\"\n",
        "\n",
        "!pip install transformers\n",
        "!apt install git make curl xz-utils file\n",
        "!apt install mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.996.5\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "make is already the newest version (4.1-9.1ubuntu1).\n",
            "xz-utils is already the newest version (5.2.2-1.3).\n",
            "curl is already the newest version (7.58.0-2ubuntu3.9).\n",
            "file is already the newest version (1:5.32-2ubuntu0.4).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.7).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libmecab-dev is already the newest version (0.996-5).\n",
            "mecab is already the newest version (0.996-5).\n",
            "mecab-ipadic is already the newest version (2.7.0-20070801+main-1).\n",
            "mecab-ipadic-utf8 is already the newest version (2.7.0-20070801+main-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: mecab-python3==0.996.5 in /usr/local/lib/python3.6/dist-packages (0.996.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuEWSxaPz8WK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "61b51466-9bbf-4e82-bf5f-0d583b5f1b0e"
      },
      "source": [
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "import transformers\n",
        "\n",
        "#東北大学の\n",
        "pretrained_model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "\n",
        "# 事前学習済みモデルのトークナイザを使用\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model_name)\n",
        "\n",
        "# 形態素解析 (内部でMeCabを使用)\n",
        "text1 = \"今日はいい天気だね\"\n",
        "text2 = \"明日は雨がふるかもしれませんね\"\n",
        "\n",
        "print(\"text1\", tokenizer.tokenize(text1))\n",
        "print(\"text2\", tokenizer.tokenize(text2))\n",
        "\n",
        "\"\"\"\n",
        "text1 ['今日', 'は', 'いい', '天気', 'だ', 'ね']\n",
        "text2 ['明日', 'は', '雨', 'が', 'ふる', 'かも', 'しれ', 'ませ', 'ん', 'ね']\n",
        "\"\"\"\n",
        "\n",
        "# BERTに入力する形式に変換\n",
        "for_bert_inputs = tokenizer([text1, text2], padding=True, return_tensors=\"pt\")#[text1,test2]でバッチ化、ptは\n",
        "print(\"for_bert_inputs\", for_bert_inputs)\n",
        "\"\"\"\n",
        "2はCSLトークン、0はパディング、3はセパレートSEP\n",
        "for_bert_inputs {'input_ids': tensor([[    2,  3246,     9,  2575, 11385,    75,  1852,     3,     0,     0,\n",
        "             0,     0],\n",
        "        [    2, 11475,     9,  3741,    14,  8491,  4830,  6758,  6769,  1058,\n",
        "          1852,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "\"\"\"\n",
        "# input_ids: 単語をIDに変換した系列．padding済み\n",
        "# token_type_ids： 2文からなるペアを入力した場合に，1文目と2文目を区別するための系列\n",
        "# attention_mask： input_idsのpadding部分とそうでない部分を区別するための系列\n",
        "\n",
        "\n",
        "# 参考．文のペアを入れる場合の例\n",
        "text3 = \"そうかな\"\n",
        "text4 = \"違うと思います\"\n",
        "tmp = tokenizer([[text1, text2],[text3, text4]], padding=True, return_tensors=\"pt\")\n",
        "print(\"tmp\", tmp)\n",
        "\"\"\"\n",
        "tmp {'input_ids': tensor([[    2,  3246,     9,  2575, 11385,    75,  1852,     3, 11475,     9,\n",
        "          3741,    14,  8491,  4830,  6758,  6769,  1058,  1852,     3],\n",
        "        [    2,  1778,    29,    18,     3,  5720,    13,  2502,  2610,     3,\n",
        "             0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],#0が一文目、1が二文目という意味\n",
        "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# BERTの事前学習済みモデルをロード．BertForSequenceClassificationは1文が与えられて分類を行うクラス．num_labelsでラベル数を指定\n",
        "# 他にもいろいろ用意されている．使用例も書かれている→  https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "model = transformers.BertForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=5)\n",
        "\n",
        "out = model(input_ids=for_bert_inputs[\"input_ids\"], token_type_ids=for_bert_inputs[\"token_type_ids\"], attention_mask=for_bert_inputs[\"attention_mask\"])\n",
        "print(\"out\", out)\n",
        "\n",
        "model = transformers.BertModel.from_pretrained(pretrained_model_name)\n",
        "out = model(input_ids=for_bert_inputs[\"input_ids\"], token_type_ids=for_bert_inputs[\"token_type_ids\"], attention_mask=for_bert_inputs[\"attention_mask\"])\n",
        "# pooled_output = outputs[1]\n",
        "\n",
        "# pooled_output = self.dropout(pooled_output)\n",
        "# logits = self.classifier(pooled_output)\n",
        "print(\"out\", out[1].size())\n",
        "\"\"\"\n",
        "out (tensor([[-0.4694, -0.2888,  0.1584,  0.1443,  0.2474],\n",
        "        [-0.4795, -0.2614,  0.0915,  0.1614,  0.1209]],\n",
        "       grad_fn=<AddmmBackward>),)\n",
        "\"\"\"\n",
        "\n",
        "# あとは損失を計算していつもどおりbackpropすればOK\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text1 ['今日', 'は', 'いい', '天気', 'だ', 'ね']\n",
            "text2 ['明日', 'は', '雨', 'が', 'ふる', 'かも', 'しれ', 'ませ', 'ん', 'ね']\n",
            "for_bert_inputs {'input_ids': tensor([[    2,  3246,     9,  2575, 11385,    75,  1852,     3,     0,     0,\n",
            "             0,     0],\n",
            "        [    2, 11475,     9,  3741,    14,  8491,  4830,  6758,  6769,  1058,\n",
            "          1852,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "tmp {'input_ids': tensor([[    2,  3246,     9,  2575, 11385,    75,  1852,     3, 11475,     9,\n",
            "          3741,    14,  8491,  4830,  6758,  6769,  1058,  1852,     3],\n",
            "        [    2,  1778,    29,    18,     3,  5720,    13,  2502,  2610,     3,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "out (tensor([[-0.3551, -0.1342, -0.0980, -0.4630, -0.1572],\n",
            "        [-0.3949, -0.1713, -0.0507, -0.3602, -0.2726]],\n",
            "       grad_fn=<AddmmBackward>),)\n",
            "out torch.Size([2, 768])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nout (tensor([[-0.4694, -0.2888,  0.1584,  0.1443,  0.2474],\\n        [-0.4795, -0.2614,  0.0915,  0.1614,  0.1209]],\\n       grad_fn=<AddmmBackward>),)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcIFJgYUsCwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#データを得る\n",
        "!wget http://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "!tar xfz ldcc-20140209.tar.gz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqcqnDzx2iIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1d11ee1-5124-4110-9846-44aa85dcc791"
      },
      "source": [
        "\n",
        "category2id = {\"kaden-channel\":0, \"peachy\":1, \"sports-watch\":2, \"dokujo-tsushin\":3, \"livedoor-homme\":4, \"it-life-hack\":5, \"movie-enter\":6}\n",
        "\n",
        "# Livedoorニュースコーパスからテキスト分類用のデータを作成．\n",
        "import glob\n",
        "import random\n",
        "\n",
        "# 1行は [文][TAB][ラベル]からなる\n",
        "write_lines = []\n",
        "for d in category2id.keys():\n",
        "  for file in glob.glob(\"text/\" + d + \"/*.txt\"):\n",
        "    with open(file) as f:\n",
        "      lines = f.readlines()\n",
        "      # 最初の2行はURLと日付なので捨てる\n",
        "      for line in lines[3:]:\n",
        "        line = line.strip()\n",
        "        if len(line) > 20 and len(line) < 256 and \"http\" not in line:\n",
        "          write_lines.append(line + \"\\t\" + str(category2id[d]) + \"\\n\")\n",
        "\n",
        "random.shuffle(write_lines)\n",
        "# Train, Dev, Testの3つに分ける.\n",
        "dev = write_lines[0:2000]\n",
        "test =write_lines[2000:4000]\n",
        "train = write_lines[4000:]\n",
        "# ファイルに保存\n",
        "w = open(\"train.tsv\", \"w\")\n",
        "w.writelines(train)\n",
        "w.close()\n",
        "w = open(\"dev.tsv\", \"w\")\n",
        "w.writelines(dev)\n",
        "w.close()\n",
        "w = open(\"test.tsv\", \"w\")\n",
        "w.writelines(test)\n",
        "w.close()\n",
        "print(\"data size \", len(train), len(dev), len(test))\n",
        "\n",
        "#!head dev.tsv\n",
        "#データセットだけ作成する、Inputids、tokenids、idsの三つを返せば良い"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data size  67653 2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQoUmu_z8AS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence, pad_sequence, pack_padded_sequence\n",
        "#word2id_path = '/content/gdrive/My Drive/Colab Notebooks/kobo2020/nlp100_chap9'#todo 80.pyで保存したファイルパスを指定\n",
        "import numpy as np\n",
        "\n",
        "    \n",
        "def get_data(fname):\n",
        "    inputids_list = [] # inputidsを格納するリスト\n",
        "    tokenids_list = []#\n",
        "    attentionmask_list = []\n",
        "    ids_list = [] #label\n",
        "    i = 0\n",
        "    with open(fname) as f:\n",
        "        for line in f:\n",
        "            if not line:\n",
        "                continue\n",
        "            line = line.split('\\t')#todo lineからタイトルを取得\n",
        "            title = line[0]\n",
        "            tmp = tokenizer(title, padding=True, return_tensors=\"pt\")\n",
        "            inputids = tmp[\"input_ids\"]\n",
        "            i += 1\n",
        "            tokenids = tmp[\"token_type_ids\"]+i\n",
        "            attentionmask = tmp[\"attention_mask\"]\n",
        "            ids = float(line[1])\n",
        "            \n",
        "            inputids_list.append(inputids)\n",
        "            tokenids_list.append(tokenids)\n",
        "            attentionmask_list.append(attentionmask)\n",
        "            ids_list.append(ids)\n",
        "\n",
        "    #inputids_list = torch.from_numpy(np.array(inputids_list))\n",
        "    #tokenids_list = torch.from_numpy(np.array(token_ids_list))\n",
        "    ids_list = torch.from_numpy(np.array(ids_list))\n",
        "\n",
        "    return  inputids_list,tokenids_list,attentionmask_list,ids_list\n",
        "\n",
        "#todo データセットだけ作成する、Inputids、tokenids、idsの三つを返せば良い\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,inputids_list, tokenids_list, attentionmask_list,ids_list):\n",
        "      self.length = len(ids_list)\n",
        "      self.inputids_list = inputids_list\n",
        "      self.tokenids_list = tokenids_list\n",
        "      self.attentionmask_list = attentionmask_list\n",
        "      self.ids_list = ids_list\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputids_list[idx],self.tokenids_list[idx],attentionmask_list[idx],self.ids_list[idx]\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsjGD7stAmKk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "70207b58-8ab6-40dc-b96e-d283fb6175c6"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# データの準備\n",
        "train_path =  '/content/gdrive/My Drive/Colab Notebooks/kobo2020/dev.tsv'#todo 学習用ファイルパス\n",
        "inputids_list, token_ids_list ,attentionmask_list,ids_list = get_data(train_path)\n",
        "#print(inputids_list[0])\n",
        "\n",
        "#train_x はテキストのリスト、yはテンソルにしてもOK\n",
        "#get_dataはテキストのリストとyはテンソルを返す\n",
        "\n",
        "dataset = MyDataset(inputids_list, token_ids_list,attentionmask_list,ids_list)\n",
        "print(dataset[0])\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[    2,  6571,   486,    12,     6,    31,  8141,  7111,    14,     6,\n",
            "         12272,    45,    11,  3083,    16,    21,    10,     8,     3]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor(3., dtype=torch.float64))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnQktDQP9oDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "78294a6e-9f3e-464c-9b35-9fc029c1bfeb"
      },
      "source": [
        "\n",
        "emb_dim = 300\n",
        "hidden_dim = 50\n",
        "target_size = 4\n",
        "batch_size = 64\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# データの準備\n",
        "train_path =  '/content/gdrive/My Drive/Colab Notebooks/kobo2020/dev.tsv'#\n",
        "inputids_list, token_ids_list ,attentionmask_list,ids_list = get_data(train_path)\n",
        "\n",
        "\n",
        "dataset = MyDataset(inputids_list, token_ids_list,attentionmask_list,ids_list)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "model = transformers.BertForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=5)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train(model, train_loader, len_train):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_num = 0\n",
        "    len_loader = len(train_loader)\n",
        "\n",
        "    for data, token,attention_mask, target in train_loader:\n",
        "        print(data)\n",
        "        # GPUへ\n",
        "        data = data.to(device)\n",
        "        token = token.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(input_ids=data, token_type_ids=token, attention_mask=attention_mask)#todo modelで予測\n",
        "        loss = loss_fn(pred,target)#todo lossを計算\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss\n",
        "        pred_labels = torch.max(pred,axis = 1).indices\n",
        "        correct_num +=  (pred_labels == target).sum().item()#todo 予測の正解数をカウント\n",
        "\n",
        "    batch_loss = total_loss/len_train #バッチごとの平均ロス\n",
        "    acc = correct_num/len_train#todo 正解率\n",
        "    return batch_loss, acc\n",
        "\n",
        "\n",
        "\n",
        "len_train = len(ids_list)\n",
        "for epoch in range(30):\n",
        "    train_loss, train_acc = train(model, train_loader, len_train)\n",
        "\n",
        "    print(f\"epoch: {epoch}\")\n",
        "    print(f\"<train> Loss: {train_loss}\\tAccuracy: {train_acc}\")\n",
        "    "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-979ce412b0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mlen_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch: {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-979ce412b0b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, len_train)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mlen_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# GPUへ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 22] at entry 0 and [1, 133] at entry 2"
          ]
        }
      ]
    }
  ]
}