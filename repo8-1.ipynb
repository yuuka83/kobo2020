{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レポート8前半\n",
    "1810083 井上悠香"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#70\n",
    "import sys\n",
    "sys.path.append(' . . ')\n",
    "import gensim\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "w2v_path = './GoogleNews-vectors-negative300.bin' #todo GoogleNews-vectors-negative300.binのパスを指定\n",
    "data_path = '/Users/inoueyuuka/Desktop/kobo2020/data70/'#todo ファイル出力ディレクトリを指定\n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)#todo gensimでword2vecのモデルを読み込む\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "category2num = {\"b\": 0, \"t\": 1, \"e\": 2, \"m\": 3}\n",
    "\n",
    "# タイトルを受け取り，単語ベクトルの平均を返す関数\n",
    "def get_feature(title):\n",
    "    word_list = title.split(' ')#todo タイトルをスペースで分割\n",
    "    vec_list = []\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            vec = w2v_model[word]#todo wordを意味するベクトルを取得　これで良いはず？\n",
    "        except KeyError:\n",
    "            vec = np.zeros(300)#todo すべての要素が0のベクトルを代入 np.zeros()でいいんか？？\n",
    "        vec_list.append(vec)    \n",
    "    vec_np = np.array(vec_list) # numpyのarrayに変換\n",
    "    feature = np.mean(vec_np)# 平均ベクトルを計算\n",
    "    return feature\n",
    "\n",
    "def get_data(fname):\n",
    "    label_list = []\n",
    "    feature_list = []\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            if not line:\n",
    "                continue\n",
    "            data = line.split('\\t')\n",
    "            title = data[1]#todo dataからタイトルを取り出す\n",
    "            feature = get_feature(title) \n",
    "            feature_list.append(feature)\n",
    "            label = category2num[data[0]]\n",
    "            label_list.append(label)\n",
    "            \n",
    "    feature_list = np.array(feature_list)#追加\n",
    "    features =  torch.from_numpy(feature_list)#todo feature_listをtensorに変換\n",
    "    label_list = np.array(label_list)#追加\n",
    "    labels =  torch.from_numpy(label_list)#todo label_listをtensorに変換\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "train_x, train_y = get_data(data_path + \"train.txt\")\n",
    "valid_x, valid_y = get_data(data_path + \"valid.txt\")\n",
    "test_x, test_y = get_data(data_path + \"test.txt\")\n",
    "\n",
    "# 保存\n",
    "torch.save(train_x, data_path + \"train_x.pt\")\n",
    "torch.save(train_y, data_path + \"train_y.pt\")\n",
    "torch.save(valid_x, data_path + \"valid_x.pt\")\n",
    "torch.save(valid_y, data_path + \"valid_y.pt\")\n",
    "torch.save(test_x, data_path + \"test_x.pt\")\n",
    "torch.save(test_y, data_path + \"test_y.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#実験\n",
    "b = torch.tensor([1,2,3,4])\n",
    "a = np.array([1,2,3,4])\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#実験\n",
    "vec_list = []\n",
    "vec_list += [1,2,3]\n",
    "print(type(vec_list))\n",
    "vec_list = np.array(vec_list)\n",
    "print(type(vec_list))\n",
    "features =  torch.from_numpy(vec_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'config' from 'common' (/Users/inoueyuuka/Desktop/kobo2020/common/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9c4dc300f0e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'config' from 'common' (/Users/inoueyuuka/Desktop/kobo2020/common/__init__.py)"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common import config\n",
    "config.GPU = True\n",
    "\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity, to_gpu\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "batch_size = 20\n",
    "wordvec_size = 650\n",
    "hidden_size = 650\n",
    "time_size = 35\n",
    "lr = 20.0\n",
    "max_epoch = 40\n",
    "max_grad = 0.25\n",
    "dropout = 0.5\n",
    "\n",
    "# 学習データの読み込み\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_val, _, _ = ptb.load_data('val')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "\n",
    "if config.GPU:\n",
    "    corpus = to_gpu(corpus)\n",
    "    corpus_val = to_gpu(corpus_val)\n",
    "    corpus_test = to_gpu(corpus_test)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "best_ppl = float('inf')\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n",
    "                time_size=time_size, max_grad=max_grad)\n",
    "\n",
    "    model.reset_state()\n",
    "    ppl = eval_perplexity(model, corpus_val)\n",
    "    print('valid perplexity: ', ppl)\n",
    "\n",
    "    if best_ppl > ppl:\n",
    "        best_ppl = ppl\n",
    "        model.save_params()\n",
    "    else:\n",
    "        lr /= 4.0\n",
    "        optimizer.lr = lr\n",
    "\n",
    "    model.reset_state()\n",
    "    print('-' * 50)\n",
    "\n",
    "\n",
    "# テストデータでの評価\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('test perplexity: ', ppl_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
