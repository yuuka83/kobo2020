{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_embedding_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDPjS8kKrbrb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "59abba68-0737-4c48-bb8b-afcd506de155"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd '/content/gdrive/My Drive/Colab Notebooks/kobo2020'\n",
        "%ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks/kobo2020\n",
            "AttentionSeq2seq.pkl                neko.txt\n",
            "attention_seq2seq.py                nlp100_chap9\n",
            "BetterRnnlm.pkl                     nlp100_chap9.pkl\n",
            "better_rnnlm.py                     nlp100_chap9.txt\n",
            "cbow_params.pkl                     \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
            "cbow.py                             PyTorchTutorial1.ipynb\n",
            "\u001b[01;34mch04\u001b[0m/                               repo10.ipynb\n",
            "\u001b[01;34mch06\u001b[0m/                               repo10test.ipynb\n",
            "\u001b[01;34mch07\u001b[0m/                               repo11.ipynb\n",
            "ch07＿pra.ipynb                     repo11_test.ipynb\n",
            "\u001b[01;34mch08\u001b[0m/                               repo12.ipynb\n",
            "ch08_pra.ipynb                      repo6.ipynb\n",
            "\u001b[01;34mcommon\u001b[0m/                             repo7.ipynb\n",
            "\u001b[01;34mdata70\u001b[0m/                             repo8.ipynb\n",
            "\u001b[01;34mdatarepo11\u001b[0m/                         repo9.ipynb\n",
            "\u001b[01;34mdataset\u001b[0m/                            simple_rnnlm.py\n",
            "dev.tsv                             \u001b[01;34msmall_parallel_enja\u001b[0m/\n",
            "enword2id.pickle                    test_embedding_lstm.ipynb\n",
            "GoogleNews-vectors-negative300.bin  test.tsv\n",
            "jaword2id.pickle                    \u001b[01;34mtext\u001b[0m/\n",
            "ldcc-20140209.tar.gz                train.tsv\n",
            "negative_sampling_layer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gSwX3fasDzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence, pack_padded_sequence, pad_sequence\n",
        "import re\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "ja_word2id_path = \"jaword2id.pickle\"\n",
        "en_word2id_path = \"enword2id.pickle\"\n",
        "ja_word2id = pickle.load(open(ja_word2id_path, \"rb\"))\n",
        "en_word2id = pickle.load(open(en_word2id_path, \"rb\"))\n",
        "\n",
        "#言語に応じたデータを取り出してidのリストを返す\n",
        "def get_ids(word_list, lang):\n",
        "    if lang == \"ja\":\n",
        "        word2id = ja_word2id\n",
        "    else:\n",
        "        word2id = en_word2id\n",
        "\n",
        "    ids = []\n",
        "    for word in word_list:\n",
        "        if word in word2id:\n",
        "            ids.append(word2id[word])\n",
        "        else:\n",
        "            ids.append(word2id[\"[UNK]\"])\n",
        "    return ids\n",
        "\n",
        "\n",
        "# 使用するデータの英語と日本語の文に応じたid列のテンソル が入ったリストを返す\n",
        "def get_data(ja_fname, en_fname):\n",
        "    ja_ids_list = []\n",
        "    en_ids_list = []\n",
        "\n",
        "    with open(ja_fname) as f:\n",
        "        for line in f:\n",
        "            if not line:\n",
        "                continue\n",
        "            word_list = line.split()\n",
        "            ids = get_ids(word_list, \"ja\")\n",
        "            ja_ids_list.append(torch.tensor(ids))\n",
        "\n",
        "    with open(en_fname) as f:\n",
        "        for line in f:\n",
        "            if not line:\n",
        "                continue\n",
        "            word_list = line.split()\n",
        "            #print(word_list)\n",
        "            ids = get_ids(word_list, \"en\")\n",
        "            en_ids_list.append(torch.tensor(ids))\n",
        "\n",
        "    return ja_ids_list, en_ids_list\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQVcM6borcIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "e8637e87-a06c-40e2-cca6-f38757fe4d19"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence, pack_padded_sequence, pad_sequence\n",
        "\n",
        "ja_data, en_data = get_data(\"small_parallel_enja/train.ja\", \"small_parallel_enja/train.en\")\n",
        "\n",
        "ja_one_text = ja_data[0]\n",
        "en_one_text = ja_data[0]\n",
        "print(ja_one_text)\n",
        "emb = nn.Embedding(10000,5) #辞書の単語数 ,隠れ層のサイズ \n",
        "#print(len(ja_one_text))#16\n",
        "out = emb(ja_one_text)#例) \n",
        "#print(out.shape)#16x5\n",
        "print(out)#単語ベクトルを隠れ層のサイズに合わせて生成？　テンソルの中の一行が一単語に対応？\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 91,  13, 229,   6, 154,  28,  21,  17,   6,   4, 276,  37,  20,  40,\n",
            "         29,   3])\n",
            "tensor([[ 1.3531,  1.6825,  0.3928,  1.0511,  1.6332],\n",
            "        [-0.1801,  1.9428, -1.1983, -0.1217, -0.6933],\n",
            "        [-0.0738,  0.2610, -1.4792, -1.3616,  0.4303],\n",
            "        [-1.3940,  0.6831,  0.1174, -1.0609, -0.8274],\n",
            "        [ 0.1608,  0.4608, -1.5504, -0.1069,  1.7522],\n",
            "        [-0.1669, -0.3400,  0.8054, -0.3359, -0.8326],\n",
            "        [ 0.3873,  0.2671,  0.7411,  0.8028,  0.8499],\n",
            "        [ 0.3686,  0.1633, -0.1061, -0.8438, -2.0437],\n",
            "        [-1.3940,  0.6831,  0.1174, -1.0609, -0.8274],\n",
            "        [ 0.6816,  1.3218,  0.1829, -0.4052, -1.3517],\n",
            "        [-1.7152,  2.2548, -1.7128, -1.2368, -1.3583],\n",
            "        [ 0.6295, -1.7423, -1.3730,  1.4075, -0.9732],\n",
            "        [ 0.3860, -0.8883, -0.9921,  0.5602, -1.5130],\n",
            "        [-0.9949, -0.9064, -1.7262, -0.2596,  1.1090],\n",
            "        [ 1.1538,  2.4834,  1.7380,  0.7371, -0.0558],\n",
            "        [ 1.0355,  0.3189,  0.5838,  0.3522,  0.3494]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMXcjK0ssx_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "5f4cb4eb-57d3-4001-fde9-3fc67fda7f09"
      },
      "source": [
        "a = ja_data[0]\n",
        "b = ja_data[1]\n",
        "ja_two_text = torch.cat([a,b],dim = 0)\n",
        "\n",
        "print(ja_two_text,ja_two_text.shape)#文くっつけちゃったから30単語のランダムな単語ベクトルが出力されている？？\n",
        "out2 = emb(ja_two_text)\n",
        "print(out2,out2.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 91,  13, 229,   6, 154,  28,  21,  17,   6,   4, 276,  37,  20,  40,\n",
            "         29,   3, 287,   9, 700,  13, 701,   6,  34,  19,  10,   2,  51,  53,\n",
            "          7,   3]) torch.Size([30])\n",
            "tensor([[ 1.3531,  1.6825,  0.3928,  1.0511,  1.6332],\n",
            "        [-0.1801,  1.9428, -1.1983, -0.1217, -0.6933],\n",
            "        [-0.0738,  0.2610, -1.4792, -1.3616,  0.4303],\n",
            "        [-1.3940,  0.6831,  0.1174, -1.0609, -0.8274],\n",
            "        [ 0.1608,  0.4608, -1.5504, -0.1069,  1.7522],\n",
            "        [-0.1669, -0.3400,  0.8054, -0.3359, -0.8326],\n",
            "        [ 0.3873,  0.2671,  0.7411,  0.8028,  0.8499],\n",
            "        [ 0.3686,  0.1633, -0.1061, -0.8438, -2.0437],\n",
            "        [-1.3940,  0.6831,  0.1174, -1.0609, -0.8274],\n",
            "        [ 0.6816,  1.3218,  0.1829, -0.4052, -1.3517],\n",
            "        [-1.7152,  2.2548, -1.7128, -1.2368, -1.3583],\n",
            "        [ 0.6295, -1.7423, -1.3730,  1.4075, -0.9732],\n",
            "        [ 0.3860, -0.8883, -0.9921,  0.5602, -1.5130],\n",
            "        [-0.9949, -0.9064, -1.7262, -0.2596,  1.1090],\n",
            "        [ 1.1538,  2.4834,  1.7380,  0.7371, -0.0558],\n",
            "        [ 1.0355,  0.3189,  0.5838,  0.3522,  0.3494],\n",
            "        [ 1.0486, -1.4149,  0.1793,  0.4084,  0.4606],\n",
            "        [-0.0401, -1.6730, -0.5241,  0.8421,  0.4686],\n",
            "        [-0.0552, -0.4158,  0.7624, -2.0452,  0.0999],\n",
            "        [-0.1801,  1.9428, -1.1983, -0.1217, -0.6933],\n",
            "        [ 0.4792, -0.1629, -0.2960,  0.2373,  0.0823],\n",
            "        [-1.3940,  0.6831,  0.1174, -1.0609, -0.8274],\n",
            "        [-0.0287, -0.8076,  0.1359,  1.9658, -0.7451],\n",
            "        [ 0.3840,  1.9427,  1.0556, -0.3989, -0.0244],\n",
            "        [-0.0376,  1.0358,  0.9786,  0.7148,  0.1247],\n",
            "        [-1.4652, -0.0392, -0.4217, -1.5935, -0.0847],\n",
            "        [-1.2362,  0.1047,  1.0953,  0.1213,  0.4493],\n",
            "        [-0.9334, -0.1406, -0.7969,  0.0486,  1.0828],\n",
            "        [ 0.4766,  0.3988,  0.3418, -1.6993,  1.2790],\n",
            "        [ 1.0355,  0.3189,  0.5838,  0.3522,  0.3494]],\n",
            "       grad_fn=<EmbeddingBackward>) torch.Size([30, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3mZv-PNumfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "ab8a6bfb-1f59-46ea-f168-03e7de68eb33"
      },
      "source": [
        "ja_two_tensor = torch.tensor([[91,13,229,6,154,28,21,17,6,4,3],[1,2,3,4,5,6,7,8,9,10,11]])#二文をまとめたバッチ？ 　　2x11 文の数x単語数\n",
        "print(ja_two_tensor,ja_two_tensor.shape)\n",
        "out2 = emb(ja_two_tensor)\n",
        "print(out2,out2.shape)#2x11x5　で、\n",
        "\"\"\"\n",
        "[単語, 単語,単語, ... ,単語]←文 [[単語, 単語,単語, ... ,単語],[単語, 単語,単語, ... ,単語]] ←文x2を\n",
        "[[[1xHの形状のランダムな単語ベクトル], [1xHの形状のランダムな単語ベクトル], ... ,[1xHの形状のランダムな単語ベクトル]],[[1xHの形状のランダムな単語ベクトル], [1xHの形状のランダムな単語ベクトル], ... ,[1xHの形状のランダムな単語ベクトル]]]\n",
        "に変換してる？\n",
        "\"\"\"\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 91,  13, 229,   6, 154,  28,  21,  17,   6,   4,   3],\n",
            "        [  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11]]) torch.Size([2, 11])\n",
            "tensor([[[ 1.3531,  1.6825,  0.3928,  1.0511,  1.6332],\n",
            "         [-0.1801,  1.9428, -1.1983, -0.1217, -0.6933],\n",
            "         [-0.0738,  0.2610, -1.4792, -1.3616,  0.4303],\n",
            "         [-1.3940,  0.6831,  0.1174, -1.0609, -0.8274],\n",
            "         [ 0.1608,  0.4608, -1.5504, -0.1069,  1.7522],\n",
            "         [-0.1669, -0.3400,  0.8054, -0.3359, -0.8326],\n",
            "         [ 0.3873,  0.2671,  0.7411,  0.8028,  0.8499],\n",
            "         [ 0.3686,  0.1633, -0.1061, -0.8438, -2.0437],\n",
            "         [-1.3940,  0.6831,  0.1174, -1.0609, -0.8274],\n",
            "         [ 0.6816,  1.3218,  0.1829, -0.4052, -1.3517],\n",
            "         [ 1.0355,  0.3189,  0.5838,  0.3522,  0.3494]],\n",
            "\n",
            "        [[-0.9075, -0.6647, -0.6396, -0.2718,  0.7547],\n",
            "         [-1.4652, -0.0392, -0.4217, -1.5935, -0.0847],\n",
            "         [ 1.0355,  0.3189,  0.5838,  0.3522,  0.3494],\n",
            "         [ 0.6816,  1.3218,  0.1829, -0.4052, -1.3517],\n",
            "         [-0.4958, -0.7398,  1.3414,  0.9384,  1.6636],\n",
            "         [-1.3940,  0.6831,  0.1174, -1.0609, -0.8274],\n",
            "         [ 0.4766,  0.3988,  0.3418, -1.6993,  1.2790],\n",
            "         [ 0.4475,  0.4793,  1.7098,  0.2127, -0.3467],\n",
            "         [-0.0401, -1.6730, -0.5241,  0.8421,  0.4686],\n",
            "         [-0.0376,  1.0358,  0.9786,  0.7148,  0.1247],\n",
            "         [-0.6266, -1.5760,  0.7292, -1.0630, -0.6153]]],\n",
            "       grad_fn=<EmbeddingBackward>) torch.Size([2, 11, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyKOCNiXvigB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6c5ec976-41ac-4260-8a81-d6775b75de4c"
      },
      "source": [
        "x = torch.tensor([[1, 2], [3, 4.]])\n",
        "print(x)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xwj9oP9vv1N",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}